# Automated Testing Coverage in LLMs

A comprehensive evaluation framework for assessing test coverage of code solutions generated by Large Language Models (LLMs) on HumanEval benchmark problems.

## Overview

This project evaluates code solutions generated by different LLMs using various prompting strategies. It measures test coverage, generates coverage reports, and assesses the fault detection capabilities of test suites.

## Models and Strategies

The project evaluates solutions from:
- **CodeLlama-7B** (Chain of Thought and Self-Planning)
- **DeepSeek-Coder-6.7B** (Chain of Thought and Self-Planning)

## Project Structure

```
├── part1_json_from_ex1/          # Initial evaluation results from JSONL files
├── part2/                         # Coverage analysis and test suite generation
├── part3/                         # Fault detection evaluation
├── solutions/                     # Code solutions generated by LLMs
├── tests/                         # Test suites for HumanEval problems
├── scripts/                       # Utility scripts for coverage analysis
├── coverage_reports/              # Generated HTML coverage reports
└── baseline_coverage_all_40.md   # Summary of coverage results
```

## Key Components

### Part 1: Baseline coverage

### Part 2: LLM-Assisted Test Generation & Coverage Improvement

### Part 3: Fault Detection Check

## Usage

### Running Coverage Analysis

```bash
python scripts/run_all_coverage.py
```

This script:
- Runs coverage analysis for all 40 solutions
- Generates HTML coverage reports
- Creates detailed coverage metrics

### Running Tests

```bash
pytest tests/ -v
```

### Viewing Coverage Reports

Coverage reports are available in the `coverage_reports/` directory, organized by model and prompting strategy.

## Requirements

- Python 3.7+
- pytest
- pytest-cov
- coverage
- human-eval

Install dependencies:
```bash
pip install pytest pytest-cov coverage human-eval
```

## Results

See `baseline_coverage_all_40.md` for a summary of coverage results across all evaluated solutions.

## HumanEval Problems

The project evaluates solutions for the following HumanEval problems:
- HumanEval/0, HumanEval/1, HumanEval/2, HumanEval/10, HumanEval/11
- HumanEval/12, HumanEval/20, HumanEval/25, HumanEval/31, HumanEval/37

## License

This project is part of CS520 coursework.